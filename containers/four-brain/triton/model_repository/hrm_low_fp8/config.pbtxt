name: "hrm_low_fp8"
backend: "tensorrt"
max_batch_size: 8
input [
  { name: "input_ids"      data_type: TYPE_INT64 dims: [-1] },
  { name: "attention_mask" data_type: TYPE_INT64 dims: [-1] }
]
output [ { name: "logits" data_type: TYPE_FP16 dims: [-1] } ]
instance_group [ { kind: KIND_GPU count: 1 } ]
dynamic_batching { preferred_batch_size: [2,4,8] max_queue_delay_microseconds: 2000 }
parameters: { key: "precision_mode" value: { string_value: "FP16" } }
optimization {
  execution_accelerators { gpu_execution_accelerator: [ { name: "tensorrt" } ] }
}
# Optimization profiles (min/opt/max) â€” HRM Low FP8
profile {
  name: "profile_0"
  inputs: { key: "input_ids"      value: { min: [1,128], opt: [4,256], max: [8,512] } }
  inputs: { key: "attention_mask" value: { min: [1,128], opt: [4,256], max: [8,512] } }
}
# FP8 target; outputs surfaced as FP16.
# Blackwell note: test FP8 engines for HRM Low after embedding/reranker are validated.
