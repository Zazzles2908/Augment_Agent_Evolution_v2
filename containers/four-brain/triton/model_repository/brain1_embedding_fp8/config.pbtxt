name: "brain1_embedding_fp8"
backend: "tensorrt"
max_batch_size: 16
input [
  { name: "input_ids"      data_type: TYPE_INT64 dims: [-1] },
  { name: "attention_mask" data_type: TYPE_INT64 dims: [-1] }
]
output [ { name: "embedding" data_type: TYPE_FP16 dims: [-1] } ]
instance_group [ { kind: KIND_GPU count: 1 } ]
dynamic_batching { preferred_batch_size: [2,4,8] max_queue_delay_microseconds: 2000 }
parameters: { key: "precision_mode" value: { string_value: "FP16" } }
optimization {
  execution_accelerators { gpu_execution_accelerator: [ { name: "tensorrt" } ] }
}
# Optimization profiles (min/opt/max) â€” Embedding FP8
profile {
  name: "profile_0"
  inputs: { key: "input_ids"      value: { min: [1,128], opt: [4,256], max: [8,512] } }
  inputs: { key: "attention_mask" value: { min: [1,128], opt: [4,256], max: [8,512] } }
}
# FP8 target: provide prebuilt FP8 plan (model.plan) or TRT calibration; keep outputs FP16.
# Blackwell note: verify FP8 kernel availability in TensorRT 10.13.x; fallback FP16 if unsupported.
